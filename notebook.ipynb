{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üç´ Building a Controlnet pipeline for interior design with Fondant\n",
    "\n",
    "This example demonstrates an end-to-end fondant pipeline to collect and process data for the fine-tuning of a [ControlNet](https://github.com/lllyasviel/ControlNet) model, focusing on images related to interior design.\n",
    "\n",
    "\n",
    "### Pipeline overview\n",
    "\n",
    "\n",
    "There are 5 components in total, these are:\n",
    "\n",
    "1. [**Prompt Generation**](components/generate_prompts): This component generates a set of seed prompts using a rule-based approach that combines various rooms and styles together, like ‚Äúa photo of a {room_type} in the style of {style_type}‚Äù. As input, it takes in a list of room types (bedroom, kitchen, laundry room, ..), a list of room styles (contemporary, minimalist, art deco, ...) and a list of prefixes (comfortable, luxurious, simple). These lists can be easily adapted to other domains. The output of this component is a list of seed prompts.\n",
    "\n",
    "2. [**Image URL Retrieval**](https://github.com/ml6team/fondant/tree/main/components/prompt_based_laion_retrieval): This component retrieves images from the [LAION-5B](https://laion.ai/blog/laion-5b/) dataset based on the seed prompts. The retrieval itself is done based on CLIP embeddings similarity between the prompt sentences and the captions in the LAION dataset. This component doesn‚Äôt return the actual images yet, only the URLs. The next component in the pipeline will then download these images.\n",
    "\n",
    "3. [**Download Images**](https://github.com/ml6team/fondant/tree/main/components/download_images): This component downloads the actual images based on the URLs retrieved by the previous component. It takes in the URLs as input and returns the actual images, along with some metadata (like their height and width).\n",
    "\n",
    "4. [**Add Captions**](https://github.com/ml6team/fondant/tree/main/components/caption_images): This component captions all images using [BLIP](https://huggingface.co/docs/transformers/model_doc/blip). This model takes in the image and generates a caption that describes the content of the image. This component takes in a Hugging Face model ID, so it can use any [Hugging Face Hub model](https://huggingface.co/models).\n",
    "\n",
    "5. [**Add Segmentation Maps**](https://github.com/ml6team/fondant/tree/main/components/segment_images): This component segments the images using the [UPerNet](https://huggingface.co/docs/transformers/model_doc/upernet) model. Each segmentation map contains segments of 150 possible categories listed [here](https://huggingface.co/openmmlab/upernet-convnext-small/blob/main/config.json#L110)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisite:**\n",
    "\n",
    "- Ensure Python version 3.8 to 3.10 is installed on your system.\n",
    "- Install and configure Docker on your system.\n",
    "- Ensure that you have a GPU for running the GPU-based component of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your environment \n",
    "!pip install \"fondant[docker]==0.6.2\" -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the pipeline\n",
    "\n",
    "First of all, we need to initialize the pipeline, which includes specifying a name for your pipeline, providing a description, and setting a base_path. The base_path is used to store the pipeline artifacts and data generated by the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "import fsspec\n",
    "\n",
    "from fondant.pipeline import ComponentOp, Pipeline\n",
    "from fondant.compiler import DockerCompiler\n",
    "from fondant.runner import DockerRunner\n",
    "\n",
    "# General configs\n",
    "BASE_PATH = \"./data_dir\"\n",
    "N_ROWS_TO_LOAD = 10  # Set to None to load all rows\n",
    "\n",
    "# Create data directory if it doesn't exist and if it's a local path\n",
    "if fsspec.core.url_to_fs(BASE_PATH)[0].protocol == ('file', 'local'):\n",
    "    Path(BASE_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    pipeline_name=\"controlnet-pipeline\",\n",
    "    pipeline_description=\"Pipeline that collects data to train ControlNet\",\n",
    "    base_path=BASE_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start off, we will use the `generate_prompts` component to generate our seed prompts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prompts_op = ComponentOp(\n",
    "    component_dir=\"components/generate_prompts\",\n",
    "    arguments={\"n_rows_to_load\": N_ROWS_TO_LOAD},\n",
    ")\n",
    "\n",
    "pipeline.add_op(generate_prompts_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our pipeline consists of a single component that loads the dataset from HuggingFace Hub. We can proceed to add the other components. All of them are reusable components, and we can initialize them using the `ComponentOp.from_registry(...)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The public clip service can only handle a few requets at a time, if you run into [timeout issues](https://github.com/rom1504/clip-retrieval/issues/267) then you might want to host your own clip service following this [guide](https://github.com/rom1504/clip-retrieval/blob/main/docs/laion5B_h14_back.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laion_retrieval_op = ComponentOp.from_registry(\n",
    "    name=\"prompt_based_laion_retrieval\",\n",
    "    arguments={\n",
    "        \"num_images\": 3,\n",
    "        \"aesthetic_score\": 9,\n",
    "        \"aesthetic_weight\": 0.5,\n",
    "        \"url\": \"https://knn.laion.ai/knn-service\"\n",
    "    },\n",
    ")\n",
    "download_images_op = ComponentOp.from_registry(\n",
    "    name=\"download_images\",\n",
    "    arguments={\n",
    "        \"timeout\": 1,\n",
    "        \"retries\": 0,\n",
    "        \"image_size\": 512,\n",
    "        \"resize_mode\": \"center_crop\",\n",
    "        \"resize_only_if_bigger\": False,\n",
    "        \"min_image_size\": 0,\n",
    "        \"max_aspect_ratio\": 2.5,\n",
    "    },\n",
    ")\n",
    "caption_images_op = ComponentOp.from_registry(\n",
    "    name=\"caption_images\",\n",
    "    arguments={\n",
    "        \"model_id\": \"Salesforce/blip-image-captioning-base\",\n",
    "        \"batch_size\": 8,\n",
    "        \"max_new_tokens\": 50,\n",
    "    },\n",
    "    number_of_accelerators=1,\n",
    "    accelerator_name=\"GPU\",\n",
    ")\n",
    "segment_images_op = ComponentOp.from_registry(\n",
    "    name=\"segment_images\",\n",
    "    arguments={\n",
    "        \"model_id\": \"openmmlab/upernet-convnext-small\",\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    number_of_accelerators=1,\n",
    "    accelerator_name=\"GPU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the components in our pipeline. It is important to note that we will define dependencies between the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(laion_retrieval_op, dependencies=generate_prompts_op)\n",
    "pipeline.add_op(download_images_op, dependencies=laion_retrieval_op)\n",
    "pipeline.add_op(caption_images_op, dependencies=download_images_op)\n",
    "pipeline.add_op(segment_images_op, dependencies=caption_images_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: writing the dataset to HF hub "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the final dataset to HF hub, we will use the `write_to_hf_hub` component. This component is a reusable Fondant component which is **generic**. This implies that we still need to customize the component specification file. We have to modify the dataframe schema defined in the `consumes` section of the component.\n",
    "\n",
    "To achieve this, we can create a `fondant_component.yaml` file in the directory `components/write_to_hf_hub` with the following content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/write_to_hub_controlnet/fondant_component.yaml\n",
    "\n",
    "name: Write to hub\n",
    "description: Component that writes a dataset to the hub\n",
    "image: fndnt/write_to_hf_hub:0.6.2\n",
    "\n",
    "consumes:\n",
    "  images:\n",
    "    fields:\n",
    "      data:\n",
    "        type: binary\n",
    "\n",
    "  captions:\n",
    "    fields:\n",
    "      text:\n",
    "        type: string\n",
    "\n",
    "  segmentations:\n",
    "    fields:\n",
    "      data:\n",
    "        type: binary\n",
    "\n",
    "args:\n",
    "  hf_token:\n",
    "    description: The hugging face token used to write to the hub\n",
    "    type: str\n",
    "  username:\n",
    "    description: The username under which to upload the dataset\n",
    "    type: str\n",
    "  dataset_name:\n",
    "    description: The name of the dataset to upload\n",
    "    type: str\n",
    "  image_column_names:\n",
    "    description: A list containing the image column names. Used to format to image to HF hub format\n",
    "    type: list\n",
    "    default: []\n",
    "  column_name_mapping:\n",
    "    description: Mapping of the consumed fondant column names to the written hub column names\n",
    "    type: dict\n",
    "    default: {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = \n",
    "HF_TOKEN = \n",
    "\n",
    "write_to_hub_controlnet = ComponentOp(\n",
    "    component_dir=\"components/write_to_hub_controlnet\",\n",
    "    arguments={\n",
    "        \"username\": USERNAME ,\n",
    "        \"hf_token\": HF_TOKEN ,\n",
    "        \"dataset_name\": \"controlnet-interior-design\",\n",
    "        \"image_column_names\": [\"images_data\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.add_op(write_to_hub_controlnet, dependencies=segment_images_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the pipeline\n",
    "\n",
    "The pipeline will generate the prompts, retreive matching images in the laion dataset and download then and finally will generate corresponding captions and segmentations needed before writing the dataset to the HF hub.\n",
    "\n",
    "We can execute our pipeline. Fondant provides various executors, and in this case, we are using the `DockerRunner` for local execution, which utilizes docker-compose under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler = DockerCompiler()\n",
    "runner = DockerRunner()\n",
    "\n",
    "compiler.compile(pipeline=pipeline, output_path = \"docker-compose.yml\")\n",
    "DockerRunner().run(\"docker-compose.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also explore the dataset using the fondant explorer, this enables you to visualize your output dataset at each component step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fondant.explore import run_explorer_app\n",
    "\n",
    "run_explorer_app(\n",
    "    base_path=BASE_PATH,\n",
    "    container=\"fndnt/data_explorer\",\n",
    "    tag=\"latest\",\n",
    "    port=8501,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
